```{python}

from transformers import AutoTokenizer, AutoModel
import torch
from datasets import Dataset, load_from_disk
from multiprocess import set_start_method
import pandas as pd
import numpy as np
import re
import ast

model_ckpt = "sentence-transformers/multi-qa-mpnet-base-dot-v1"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt, batched=True)
model = AutoModel.from_pretrained(model_ckpt)

device = torch.device("cuda")
model.to(device)

def cls_pooling(model_output):
    return model_output.last_hidden_state[:, 0]


def get_embeddings(chunks):
    embeddings = []
    for text_list in chunks:
        encoded_input = tokenizer(
            text_list, padding=True, truncation=True, return_tensors="pt"
        )
        encoded_input = {k: v.to(device) for k, v in encoded_input.items()}
        model_output = model(**encoded_input)
        embeddings.append(cls_pooling(model_output).detach().cpu().numpy()[0])
    return np.array(embeddings)
```

### Constituent assembly debates
```{python}
debateContent = pd.read_csv("./data/constituent-assembly/debates.csv")


df = Dataset.from_pandas(debateContent)
embeddings_dataset = df.map(
    lambda x: {"embeddings": get_embeddings(x["content"]).detach().cpu().numpy()[0]}
)
embeddings_dataset.save_to_disk("./data/constituent-assembly/debates-embeddings")
```

### News
```{python}
hwdb = pd.read_csv("./data/hwdb/HWdb_2024_geocoded.csv")
hwdf = Dataset.from_pandas(hwdb)
hwdfEmbeddings = hwdf.map(
    lambda x: {"embeddings": get_embeddings(x["content"]).detach().cpu().numpy()[0]}
)
hwdfEmbeddings.save_to_disk("./data/hwdb/hwdb-embeddings")
```

### Lok Sabha QandA
```{python}

sabhaQuestions15 = pd.read_csv("./data/sabha/activity/Questions/Lok Sabha/15th_wText.csv")
sabhaQuestions16 = pd.read_csv("./data/sabha/activity/Questions/Lok Sabha/16th_wText.csv")
sabhaQuestins17 = pd.read_csv("./data/sabha/activity/Questions/Lok Sabha/17th_wText.csv")

sabhaQuestions = pd.concat(
    [sabhaQuestions15, sabhaQuestions16, sabhaQuestins17], ignore_index=True
)

sabhaQuestions["questionAnswer"] = sabhaQuestions["questionAnswer"].apply(
    lambda x: re.sub(r'[^a-zA-Z0-9\s.,!?;:\'"()-]+', "", str(x))
)

def separateQandA(x):

    result = re.findall( r"GOVERNMENT OF INDIA(.*)Answer(.*)", x, re.DOTALL)
    print(result.groups())
    question = result.group(0) if result.group(0) else ""
    answer = result.group(1) if result.group(1) else ""

    return question, answer
sabhaQuestions['Question'], sabhaQuestions['Answer'] = sabhaQuestions['questionAnswer'].apply(separateQandA)

# sabhaQuestions = Dataset.from_pandas(sabhaQuestions)

# sabhaQuestionsEmbeddings = sabhaQuestions.map(
#     lambda x: {
#         "embeddings": get_embeddings(x["questionAnswer"]).detach().cpu().numpy()[0]
#     }
# )
# sabhaQuestionsEmbeddings.save_to_disk("./data/sabha/sabha-embeddings-english-2")
```

### Courts
```{python}
courtJudgements = pd.read_csv(
    "./data/court-judgements/constitutional_courts_cleaned.csv"
)


def parse_paragraphs(paragraphs):
    try:
        # return ast.literal_eval(paragraphs)
        return re.sub(r'[^a-zA-Z0-9\s.,!?;:\'"()-]+', "", str(paragraphs))
    except Exception as e:
        return []


# def retokenize(paragraphs):
#     for para in paragraphs:
#         if len(para) <400:

# # drop a col
# courtJudgements = courtJudgements.drop(columns=["paragraphs"])
courtJudgements["cleaned_paras"] = courtJudgements["cleaned_paras"].apply(
    parse_paragraphs
)

# courtJudgements = courtJudgements.explode("cleaned_paras").reset_index(drop=True)


courtJudgements = Dataset.from_pandas(courtJudgements)


courtJudgementsEmbeddings = courtJudgements.map(
    lambda x: {"embeddings": get_embeddings(x["paragraphs"]).detach().cpu().numpy()[0]}
)


courtJudgementsEmbeddings.save_to_disk(
    "./data/court-judgements/supreme-court-embeddings"
)
```

## Lok Sabha full day debates

### Semantic chunking

```{python}     
import pandas as pd
import re 
from tqdm import tqdm
from semantic_text_splitter import TextSplitter
from transformers import AutoTokenizer
from tokenizers import Tokenizer

import ast
tokenizer = Tokenizer.from_pretrained("bert-base-uncased")
  
#splitter = TextSplitter.from_huggingface_tokenizer(tokenizer, max_tokens)

# Maximum number of characters in a chunk. Will fill up the
# chunk until it is somewhere in this range.
splitter = TextSplitter((200,1000))


# tokenize into smaller sizes, duplicate rows, and make a db
data = "/mnt/vault/Vault/Areas/Constitutional Observer/india-loksabha-watch/data/loksabha/sabha-english-documentation-links-text.csv"

df = pd.read_csv(data)

#fill nan with empty string
df = df.fillna('')

# break the text into smaller pieces, every 200 chars and after a new line
tqdm.pandas() 
df['txt'] = df['txt'].progress_apply(lambda x: splitter.chunks(x))
# df = df.explode("txt").reset_index(drop=True)
df.head(20)

df.to_csv("data/sabha/sabha-day-english.csv", index=False)

# remove all new line characters and other characters we don't want

df = pd.read_csv("../data/sabha/sabha-day-english.csv")
df['txt'] = df['txt'].apply(lambda x: re.sub(r'[\x00-\x1f\x7f-\x9f]', "", str(x)))

# explode 
# recognise lists
df['txt'] = df['txt'].apply(lambda x: x.replace('\\n', ' '))
df['txt'] = df['txt'].apply(lambda x: ast.literal_eval(x))

dfNew = df.filter(items=['txt', 'Date'], axis=1)
dfNew = dfNew[dfNew["txt"].apply(lambda x: len(x) > 100)]
# del df
del df 

dfNew = dfNew.explode("txt").reset_index(drop=True)
dfNew = Dataset.from_pandas(dfNew)
dfNew.save_to_disk("../data/sabha/sabha-day-english-exploded")
```

```{python}

def checker(x):
    if len(re.findall(r"[a-zA-Z]+", x["txt"])) >= 50:
        return True
    else:
        return False

if __name__ == "__main__":
    dfNew = load_from_disk("../data/sabha/sabha-day-english-exploded")
    # lost all rows with less than 100 alphabets
    dfNewDropped = dfNew.filter(checker)
    sabhaJudgementsEmbeddings = dfNewDropped.map(
        lambda x: {"embeddings": get_embeddings(x["txt"])}, batched=True )
    sabhaJudgementsEmbeddings.save_to_disk(
        "../data/sabha/fullday-embeddings-english"
    )
```

### Create FAISS indices and save
```{python}
import faiss

# Create faiss indices and save
debatesEmbeddings = load_from_disk("../data/constituent-assembly/debates-embeddings-2")
debatesEmbeddings.add_faiss_index(column='embeddings', string_factory="HNSW32", train_size=10000) 
debatesEmbeddings.save_faiss_index(file ="../data/constituent-assembly/debates-embeddings-2.faiss", index_name="embeddings")


sabhaFullDayEmbeddings = load_from_disk("../data/sabha/fullday-embeddings-english")
sabhaFullDayEmbeddings.add_faiss_index(column='embeddings', string_factory="SQfp16", train_size=1000000)
sabhaFullDayEmbeddings.save_faiss_index(file ="../data/sabha/fullday-embeddings-english.faiss", index_name="embeddings")

```