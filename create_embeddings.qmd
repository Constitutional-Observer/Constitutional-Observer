```{python}
from transformers import AutoTokenizer, AutoModel
import torch
from datasets import Dataset
import pandas as pd
import re
import ast

model_ckpt = "sentence-transformers/multi-qa-mpnet-base-dot-v1"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = AutoModel.from_pretrained(model_ckpt)

device = torch.device("cuda")
model.to(device)


def cls_pooling(model_output):
    return model_output.last_hidden_state[:, 0]


def get_embeddings(text_list):
    encoded_input = tokenizer(
        text_list, padding=True, truncation=True, return_tensors="pt"
    )
    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}
    model_output = model(**encoded_input)
    return cls_pooling(model_output)
```

### Constituent assembly debates
```{python}
debateContent = pd.read_csv("./data/constituent-assembly/debates.csv")


df = Dataset.from_pandas(debateContent)
embeddings_dataset = df.map(
    lambda x: {"embeddings": get_embeddings(x["content"]).detach().cpu().numpy()[0]}
)
embeddings_dataset.save_to_disk("./data/constituent-assembly/debates-embeddings")
```

### News
```{python}
hwdb = pd.read_csv("./data/hwdb/HWdb_2024_geocoded.csv")
hwdf = Dataset.from_pandas(hwdb)
hwdfEmbeddings = hwdf.map(
    lambda x: {"embeddings": get_embeddings(x["content"]).detach().cpu().numpy()[0]}
)
hwdfEmbeddings.save_to_disk("./data/hwdb/hwdb-embeddings")
```

### Lok Sabha   
```{python}
sabhaQuestions = pd.read_csv("./data/sabha/activity/Questions/Lok Sabha/17th_wText.csv")
sabhaQuestions["questionAnswer"] = sabhaQuestions["questionAnswer"].apply(
    lambda x: re.sub(r'[^a-zA-Z0-9\s.,!?;:\'"()-]+', "", str(x))
)
sabhaQuestions = Dataset.from_pandas(sabhaQuestions)

sabhaQuestionsEmbeddings = sabhaQuestions.map(
    lambda x: {
        "embeddings": get_embeddings(x["questionAnswer"]).detach().cpu().numpy()[0]
    }
)
sabhaQuestionsEmbeddings.save_to_disk("./data/sabha/sabha-embeddings-english-2")
```

### Courts
```{python}
courtJudgements = pd.read_csv(
    "./data/court-judgements/constitutional_courts_cleaned.csv"
)


def parse_paragraphs(paragraphs):
    try:
        # return ast.literal_eval(paragraphs)
        return re.sub(r'[^a-zA-Z0-9\s.,!?;:\'"()-]+', "", str(paragraphs))
    except Exception as e:
        return []


# def retokenize(paragraphs):
#     for para in paragraphs:
#         if len(para) <400:

# # drop a col
# courtJudgements = courtJudgements.drop(columns=["paragraphs"])
courtJudgements["cleaned_paras"] = courtJudgements["cleaned_paras"].apply(
    parse_paragraphs
)

# courtJudgements = courtJudgements.explode("cleaned_paras").reset_index(drop=True)


courtJudgements = Dataset.from_pandas(courtJudgements)


courtJudgementsEmbeddings = courtJudgements.map(
    lambda x: {"embeddings": get_embeddings(x["paragraphs"]).detach().cpu().numpy()[0]}
)


courtJudgementsEmbeddings.save_to_disk(
    "./data/court-judgements/supreme-court-embeddings"
)
```

